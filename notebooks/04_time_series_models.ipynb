{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Time Series Models for Crime Prediction\n",
                "\n",
                "This notebook implements:\n",
                "1. ARIMA model\n",
                "2. Prophet model\n",
                "3. LSTM model\n",
                "4. Model comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from statsmodels.tsa.arima.model import ARIMA\n",
                "from prophet import Prophet\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "\n",
                "print(\"Libraries loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load time series data\n",
                "ts_data = pd.read_csv('../data/processed/overall_monthly_timeseries.csv')\n",
                "ts_data['Date'] = pd.to_datetime(ts_data['Date'])\n",
                "ts_data = ts_data.sort_values('Date')\n",
                "\n",
                "print(f\"Data shape: {ts_data.shape}\")\n",
                "print(f\"Date range: {ts_data['Date'].min()} to {ts_data['Date'].max()}\")\n",
                "print(ts_data.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-test split (80-20)\n",
                "train_size = int(len(ts_data) * 0.8)\n",
                "train_data = ts_data[:train_size]\n",
                "test_data = ts_data[train_size:]\n",
                "\n",
                "print(f\"Training data: {len(train_data)} months\")\n",
                "print(f\"Testing data: {len(test_data)} months\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ARIMA Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ARIMA model\n",
                "arima_model = ARIMA(train_data['Incidents_Reported'], order=(1,1,1))\n",
                "arima_fitted = arima_model.fit()\n",
                "\n",
                "# Predictions\n",
                "arima_pred = arima_fitted.forecast(steps=len(test_data))\n",
                "\n",
                "# Metrics\n",
                "arima_mae = mean_absolute_error(test_data['Incidents_Reported'], arima_pred)\n",
                "arima_rmse = np.sqrt(mean_squared_error(test_data['Incidents_Reported'], arima_pred))\n",
                "arima_r2 = r2_score(test_data['Incidents_Reported'], arima_pred)\n",
                "\n",
                "print(\"ARIMA Model Performance:\")\n",
                "print(f\"MAE: {arima_mae:.2f}\")\n",
                "print(f\"RMSE: {arima_rmse:.2f}\")\n",
                "print(f\"R²: {arima_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prophet Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for Prophet\n",
                "prophet_train = train_data[['Date', 'Incidents_Reported']].copy()\n",
                "prophet_train.columns = ['ds', 'y']\n",
                "\n",
                "# Train Prophet model\n",
                "prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
                "prophet_model.fit(prophet_train)\n",
                "\n",
                "# Make predictions\n",
                "future = prophet_model.make_future_dataframe(periods=len(test_data), freq='MS')\n",
                "prophet_forecast = prophet_model.predict(future)\n",
                "prophet_pred = prophet_forecast.iloc[-len(test_data):]['yhat'].values\n",
                "\n",
                "# Metrics\n",
                "prophet_mae = mean_absolute_error(test_data['Incidents_Reported'], prophet_pred)\n",
                "prophet_rmse = np.sqrt(mean_squared_error(test_data['Incidents_Reported'], prophet_pred))\n",
                "prophet_r2 = r2_score(test_data['Incidents_Reported'], prophet_pred)\n",
                "\n",
                "print(\"Prophet Model Performance:\")\n",
                "print(f\"MAE: {prophet_mae:.2f}\")\n",
                "print(f\"RMSE: {prophet_rmse:.2f}\")\n",
                "print(f\"R²: {prophet_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. LSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale data\n",
                "scaler = MinMaxScaler()\n",
                "scaled_data = scaler.fit_transform(ts_data[['Incidents_Reported']])\n",
                "\n",
                "# Create sequences\n",
                "def create_sequences(data, lookback=12):\n",
                "    X, y = [], []\n",
                "    for i in range(lookback, len(data)):\n",
                "        X.append(data[i-lookback:i, 0])\n",
                "        y.append(data[i, 0])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "lookback = 12\n",
                "X, y = create_sequences(scaled_data, lookback)\n",
                "\n",
                "# Split\n",
                "train_size = int(len(X) * 0.8)\n",
                "X_train, X_test = X[:train_size], X[train_size:]\n",
                "y_train, y_test = y[:train_size], y[train_size:]\n",
                "\n",
                "# Reshape for LSTM\n",
                "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
                "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
                "\n",
                "print(f\"X_train shape: {X_train.shape}\")\n",
                "print(f\"X_test shape: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build LSTM model\n",
                "lstm_model = Sequential([\n",
                "    LSTM(50, activation='relu', return_sequences=True, input_shape=(lookback, 1)),\n",
                "    Dropout(0.2),\n",
                "    LSTM(50, activation='relu'),\n",
                "    Dropout(0.2),\n",
                "    Dense(1)\n",
                "])\n",
                "\n",
                "lstm_model.compile(optimizer='adam', loss='mse')\n",
                "print(lstm_model.summary())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train LSTM\n",
                "history = lstm_model.fit(X_train, y_train, epochs=50, batch_size=16, \n",
                "                        validation_split=0.1, verbose=0)\n",
                "\n",
                "# Plot training history\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(history.history['loss'], label='Training Loss')\n",
                "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
                "plt.title('LSTM Model Training History')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LSTM Predictions\n",
                "lstm_pred_scaled = lstm_model.predict(X_test, verbose=0)\n",
                "lstm_pred = scaler.inverse_transform(lstm_pred_scaled).flatten()\n",
                "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
                "\n",
                "# Metrics\n",
                "lstm_mae = mean_absolute_error(y_test_original, lstm_pred)\n",
                "lstm_rmse = np.sqrt(mean_squared_error(y_test_original, lstm_pred))\n",
                "lstm_r2 = r2_score(y_test_original, lstm_pred)\n",
                "\n",
                "print(\"LSTM Model Performance:\")\n",
                "print(f\"MAE: {lstm_mae:.2f}\")\n",
                "print(f\"RMSE: {lstm_rmse:.2f}\")\n",
                "print(f\"R²: {lstm_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comparison table\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['ARIMA', 'Prophet', 'LSTM'],\n",
                "    'MAE': [arima_mae, prophet_mae, lstm_mae],\n",
                "    'RMSE': [arima_rmse, prophet_rmse, lstm_rmse],\n",
                "    'R²': [arima_r2, prophet_r2, lstm_r2]\n",
                "})\n",
                "\n",
                "print(\"\\nModel Comparison:\")\n",
                "print(comparison.to_string(index=False))\n",
                "\n",
                "# Find best model\n",
                "best_model = comparison.loc[comparison['R²'].idxmax(), 'Model']\n",
                "print(f\"\\n✓ Best performing model: {best_model}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization comparison\n",
                "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
                "\n",
                "# ARIMA\n",
                "axes[0].plot(test_data['Date'].values, test_data['Incidents_Reported'].values, \n",
                "             label='Actual', marker='o', linewidth=2)\n",
                "axes[0].plot(test_data['Date'].values, arima_pred, \n",
                "             label='Predicted', marker='s', linewidth=2, alpha=0.7)\n",
                "axes[0].set_title(f'ARIMA - R²: {arima_r2:.4f}', fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Prophet\n",
                "axes[1].plot(test_data['Date'].values, test_data['Incidents_Reported'].values, \n",
                "             label='Actual', marker='o', linewidth=2)\n",
                "axes[1].plot(test_data['Date'].values, prophet_pred, \n",
                "             label='Predicted', marker='s', linewidth=2, alpha=0.7)\n",
                "axes[1].set_title(f'Prophet - R²: {prophet_r2:.4f}', fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "# LSTM\n",
                "test_dates_lstm = ts_data.iloc[train_size+lookback:]['Date'].values\n",
                "axes[2].plot(test_dates_lstm, y_test_original, \n",
                "             label='Actual', marker='o', linewidth=2)\n",
                "axes[2].plot(test_dates_lstm, lstm_pred, \n",
                "             label='Predicted', marker='s', linewidth=2, alpha=0.7)\n",
                "axes[2].set_title(f'LSTM - R²: {lstm_r2:.4f}', fontweight='bold')\n",
                "axes[2].legend()\n",
                "axes[2].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib\n",
                "\n",
                "# Save models\n",
                "arima_fitted.save('../models/arima_model.pkl')\n",
                "joblib.dump(prophet_model, '../models/prophet_model.pkl')\n",
                "lstm_model.save('../models/lstm_model.h5')\n",
                "joblib.dump(scaler, '../models/scaler.pkl')\n",
                "\n",
                "print(\"✓ All models saved successfully!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}